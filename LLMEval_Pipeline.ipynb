{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from SimilarityHelpers import get_cos_similarity_score, get_rouge_scores\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "from detect_pretraining import detect_pretraining_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Steps:\n",
    "- Load the dataset(s)\n",
    "- (Optional) Do any splitting if needed\n",
    "- (Optional) Filter any low quality input/output pairs\n",
    "- Rename all article/text/content/body columns to \"input\"\n",
    "- Rename all summary/label/output columns to \"summary\" \n",
    "\n",
    "\n",
    "Notes:\n",
    "- Renaming is done so its easier during inference time to make model calls\n",
    "- Some datasets are too large to load reliably so we split and use only a portion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69954d7cdf474f65a68eb044728755e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\color\\.cache\\huggingface\\hub\\datasets--Samsung--samsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c7a95dda254fda86771dab73915308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "samsum.py:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27efb7c504cf456c9544d2094471fd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.7z:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\color\\.cache\\huggingface\\hub\\datasets--samsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30cc07abc6b4aafaa45c417c0bdb8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6efd28e88444169fb404adf0b7723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c143d68cd3974ab08e3e9bca52a04e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum = load_dataset(\"Samsung/samsum\", trust_remote_code=True)['test']\n",
    "samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bbbd9e96254ff4996be781ab30f813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\color\\.cache\\huggingface\\hub\\datasets--webis--tldr-17. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf695cc0653e460b8e68bd12cf098b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tldr-17.py:   0%|          | 0.00/4.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c57d4f5b4745b18585a03415693861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/2.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4502b7d70c4355bee3c825499a7387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus-webis-tldr-17.zip:   0%|          | 0.00/3.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea43d7846b4d4faeb031c267bb134b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3848330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5811b7fa331b4167bc4ce667382d625e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "        num_rows: 3848330\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webis = load_dataset(\"webis/tldr-17\", trust_remote_code=True)\n",
    "webis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'content', 'summary'],\n",
       "    num_rows: 38484\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a random 10% of Webis\n",
    "webis_sel = webis['train'].shuffle(seed=random_seed).train_test_split(test_size=0.01)\n",
    "webis_test = webis_sel['test']\n",
    "webis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['author', 'body', 'normalizedBody', 'subreddit', 'subreddit_id', 'id', 'input', 'summary'],\n",
       "    num_rows: 38484\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the dataset input/output columns so it is input for the model input and summary for the gold label\n",
    "webis_test = webis_test.rename_columns({\"content\": \"input\"})\n",
    "webis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'summary'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum = samsum.rename_columns({\"dialogue\": \"input\"})\n",
    "samsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefix(Enum):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    INSTRUCTION_AT_TOP = \"Summarize the following text: \"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    \"\"\"\n",
    "    Models supported by HuggingFace we can use for testing.\n",
    "    We picked models that come in multiple sizes.\n",
    "    - HuggingFaceTB/SmolLM2-135M\n",
    "    \"\"\"\n",
    "    SMOL_LM2_135M = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "    SMOL_LM2_135M_Instruct = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    SMOL_LM2_360M_Instruct = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    SMOL_LM2_1p7B = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    SMOL_LM2_1p7B_Instruct = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    PHI_3p5_Mini_Instruct = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceInference:\n",
    "    def __init__(self, model: Model):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model.value)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model.value).to(DEVICE)\n",
    "\n",
    "        # Add a padding token to the tokenizer if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "    def predict(self,  prompt: str, options: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the output of a model given a prompt using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {}\n",
    "            \n",
    "        try:\n",
    "            # Tokenize and generate\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                temperature=options.get(\"temperature\", 0.001),\n",
    "                top_p=options.get(\"top_p\", 0.9),\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return \"None\"\n",
    "\n",
    "    def predict_batch(self, prompts: List[str], options: Optional[Dict] = {}) -> List[str]:\n",
    "        \"\"\"\n",
    "        Batch prediction using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        return [self.predict(prompt, options) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_results_row():\n",
    "    return {\n",
    "        \"Model\": None,\n",
    "        \"Task_Prefix\": None,\n",
    "        \"Dataset_Name\": None,\n",
    "        \"Model_Responses\": None,\n",
    "        \"Gold_Labels\": None,\n",
    "        \"Min_K_Responses\": None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EVALS = [\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_135M_Instruct,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_360M_Instruct,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_1p7B,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_1p7B_Instruct,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.PHI_3p5_Mini_Instruct,\n",
    "        \"Options\": {}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_EVALS = [\n",
    "    {\n",
    "        \"Name\": \"Samsum\",\n",
    "        \"Dataset\": samsum,\n",
    "    }\n",
    "#    {\n",
    "#        \"Name\": \"Webis\",\n",
    "#        \"Dataset\": webis_test,\n",
    "#    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating HuggingFaceTB/SmolLM2-135M-Instruct on Samsum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e177d5793045c4bbd7181680c8882b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\color\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM2-135M-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c64b0f752c42cabf66554c18f481d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812876b369c84b25a5493d3e3982c152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4d0198c8a14f9584886176a5f170a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06401c2e2a478581685ef538682fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cee209cc21d40fca4faf0be7303f634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5cd0074ac64b80b036e5d51c861bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c34563217e4b0c832bc3d14d49bb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataset in DATASET_EVALS:\n",
    "    for model in MODEL_EVALS:\n",
    "        # Start a timer\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        print(f\"Evaluating {model['Model']} on {dataset['Name']}\")\n",
    "\n",
    "        # Load the model\n",
    "        model_obj = HuggingFaceInference(model['Model'])\n",
    "\n",
    "        # Get the prompts\n",
    "        inputs = dataset['Dataset']['input']\n",
    "\n",
    "        # TODO: We should maybe vary this somehow\n",
    "        # Map the inputs with the task prefix\n",
    "        prompts = [TaskPrefix.INSTRUCTION_AT_TOP.value + \"\\n\" + input for input in inputs]\n",
    "\n",
    "        # Get the gold labels\n",
    "        summaries = dataset['Dataset']['summary']\n",
    "\n",
    "        # Get the model responses\n",
    "        model_responses = model_obj.predict_batch(prompts, model['Options'] if model['Options'] else {})\n",
    "\n",
    "        # Grab the min-k% responses and calculations \n",
    "        min_k_responses = detect_pretraining_batch(prompts, model['Model'])\n",
    "\n",
    "        # Save the results\n",
    "        model_results_row = create_model_results_row()\n",
    "        model_results_row['Model'] = model['Model']\n",
    "        model_results_row['Task_Prefix'] = TaskPrefix.INSTRUCTION_AT_TOP.value\n",
    "        model_results_row['Dataset_Name'] = dataset['Name']\n",
    "        model_results_row['Model_Responses'] = model_responses\n",
    "        model_results_row['Min_K_Responses'] = min_k_responses\n",
    "        \n",
    "        model_results_row['Gold_Labels'] = summaries\n",
    "\n",
    "        model_results[f\"{model['Model']}_{dataset['Name']}\"] = model_results_row\n",
    "    \n",
    "        # Print how long it took\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LLM Responses for contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_result_key in model_results:\n",
    "    model_result = model_results[model_result_key]\n",
    "    model_result['cos_sim_scores'] = []\n",
    "    model_result['rouge_sim_scores'] = []\n",
    "    for (prediction, gold_label) in zip(model_result['Model_Responses'], model_result['Gold_Labels']):\n",
    "        model_result['cos_sim_scores'].append(get_cos_similarity_score(embedder.encode(prediction), embedder.encode(gold_label)))\n",
    "        model_result['rouge_sim_scores'].append(get_rouge_scores(prediction, gold_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_result_key in model_results:\n",
    "    model_result = model_results[model_result_key]\n",
    "    df = pd.DataFrame(model_result)\n",
    "    file_name = model_result_key.replace(\"//\", \"_\").replace(\"/\", \"_\")\n",
    "    df.to_csv(f\"./output/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
