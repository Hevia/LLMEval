{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from SimilarityHelpers import get_cos_similarity_score, get_rouge_scores\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from detect_pretraining import detect_pretraining_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Steps:\n",
    "- Load the dataset(s)\n",
    "- (Optional) Do any splitting if needed\n",
    "- (Optional) Filter any low quality input/output pairs\n",
    "- Rename all article/text/content/body columns to \"input\"\n",
    "- Rename all summary/label/output columns to \"summary\" \n",
    "\n",
    "\n",
    "Notes:\n",
    "- Renaming is done so its easier during inference time to make model calls\n",
    "- Some datasets are too large to load reliably so we split and use only a portion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'summary'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum = load_dataset(\"Samsung/samsum\", trust_remote_code=True)['test']\n",
    "samsum = samsum.rename_columns({\"dialogue\": \"input\"})\n",
    "samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webis = load_dataset(\"webis/tldr-17\", trust_remote_code=True)\n",
    "\n",
    "# Take a random 10% of Webis\n",
    "webis_sel = webis['train'].shuffle(seed=random_seed).train_test_split(test_size=0.01)\n",
    "webis_test = webis_sel['test']\n",
    "\n",
    "\n",
    "# Normalize the dataset input/output columns so it is input for the model input and summary for the gold label\n",
    "webis_test = webis_test.rename_columns({\"content\": \"input\"})\n",
    "\n",
    "# Trigger garbage collection\n",
    "webis = None\n",
    "webis_sel = None\n",
    "\n",
    "webis_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefix(Enum):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    INSTRUCTION_AT_TOP = \"Summarize the following text: \"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    \"\"\"\n",
    "    Models supported by HuggingFace we can use for testing.\n",
    "    We picked models that come in multiple sizes.\n",
    "    - HuggingFaceTB/SmolLM2-135M\n",
    "    \"\"\"\n",
    "    SMOL_LM2_135M = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "    SMOL_LM2_135M_Instruct = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    SMOL_LM2_360M_Instruct = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    SMOL_LM2_1p7B = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "    SMOL_LM2_1p7B_Instruct = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    PHI_3p5_Mini_Instruct = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceInference:\n",
    "    def __init__(self, model: Model):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model.value)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model.value).to(DEVICE)\n",
    "\n",
    "        # Add a padding token to the tokenizer if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "    def predict(self,  prompt: str, options: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the output of a model given a prompt using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {}\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            # Tokenize and generate\n",
    "            input_text = self.tokenizer.apply_chat_template(messages,  tokenize=False)\n",
    "\n",
    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                temperature=options.get(\"temperature\", 0.001),\n",
    "                top_p=options.get(\"top_p\", 0.9),\n",
    "                do_sample=True,\n",
    "            )\n",
    "            \n",
    "            # Extract just the new generated text, not the original prompt\n",
    "            prompt_length = len(inputs[\"input_ids\"][0])\n",
    "            generated_ids = outputs[0][prompt_length:]\n",
    "            \n",
    "            return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return \"None\"\n",
    "\n",
    "    def predict_batch(self, prompts: List[str], options: Optional[Dict] = {}) -> List[str]:\n",
    "        \"\"\"\n",
    "        Batch prediction using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        return [self.predict(prompt, options) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = HuggingFaceInference(Model.SMOL_LM2_135M_Instruct)\n",
    "model_obj.predict(\"Capital of France?\", options={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_results_row():\n",
    "    return {\n",
    "        \"Model\": None,\n",
    "        \"Task_Prefix\": None,\n",
    "        \"Dataset_Name\": None,\n",
    "        \"Model_Responses\": None,\n",
    "        \"Gold_Labels\": None,\n",
    "        \"Min_K_Responses\": [],\n",
    "        \"cos_sim_scores\": [],\n",
    "        \"rouge_sim_scores\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_encoder(obj):\n",
    "    \"\"\"\n",
    "    Custom JSON encoder to handle types that the default JSON encoder\n",
    "    cannot handle, like NumPy arrays and PyTorch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    # If an object has a 'tolist' method, use it\n",
    "    if hasattr(obj, 'tolist'):\n",
    "        return obj.tolist()\n",
    "    # If all else fails, raise a TypeError\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(data, filename=\"output\"):\n",
    "    # Attempt to save as CSV using pandas DataFrame\n",
    "    try:\n",
    "        # Wrap the dict in a list to create a single-row DataFrame.\n",
    "        df = pd.DataFrame([data])\n",
    "        df.to_csv(f\"{filename}.csv\", index=False)\n",
    "        print(f\"Data saved as CSV: {filename}.csv\")\n",
    "        return\n",
    "    except Exception as e_csv:\n",
    "        print(\"Saving as CSV failed:\", e_csv)\n",
    "\n",
    "    # Attempt to save as JSON\n",
    "    try:\n",
    "        with open(f\"{filename}.json\", \"w\") as f:\n",
    "            json.dump(data, f, default=custom_encoder)\n",
    "        print(f\"Data saved as JSON: {filename}.json\")\n",
    "        return\n",
    "    except Exception as e_json:\n",
    "        print(\"Saving as JSON failed:\", e_json)\n",
    "\n",
    "    # Attempt to save as Pickle\n",
    "    try:\n",
    "        with open(f\"{filename}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Data saved as Pickle: {filename}.pkl\")\n",
    "    except Exception as e_pickle:\n",
    "        print(\"Saving as Pickle failed:\", e_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EVALS = [\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_1p7B_Instruct,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_EVALS = [\n",
    "    {\n",
    "        \"Name\": \"Samsum\",\n",
    "        \"Dataset\": samsum,\n",
    "    }\n",
    "#    {\n",
    "#        \"Name\": \"Webis\",\n",
    "#        \"Dataset\": webis_test,\n",
    "#    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating HuggingFaceTB/SmolLM2-135M-Instruct on Samsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\color\\Desktop\\code\\llmeval\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating cos/rouge scores for run HuggingFaceTB/SmolLM2-135M-Instruct_Samsum\n",
      "Attempting to save file for run HuggingFaceTB/SmolLM2-135M-Instruct_Samsum....\n",
      "Data saved as CSV: ./output//HuggingFaceTB_SmolLM2-135M-Instruct_Samsum.csv\n",
      "Execution time: 560.8246450000006 seconds for run HuggingFaceTB/SmolLM2-135M-Instruct_Samsum\n",
      "Evaluating HuggingFaceTB/SmolLM2-360M-Instruct on Samsum\n",
      "Now calculating cos/rouge scores for run HuggingFaceTB/SmolLM2-360M-Instruct_Samsum\n",
      "Attempting to save file for run HuggingFaceTB/SmolLM2-360M-Instruct_Samsum....\n",
      "Data saved as CSV: ./output//HuggingFaceTB_SmolLM2-360M-Instruct_Samsum.csv\n",
      "Execution time: 598.8580187000007 seconds for run HuggingFaceTB/SmolLM2-360M-Instruct_Samsum\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASET_EVALS:\n",
    "    for model in MODEL_EVALS:\n",
    "        run_key = f\"{model['Model']}_{dataset['Name']}\"\n",
    "\n",
    "        # Start a timer\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        print(f\"Evaluating {model['Model']} on {dataset['Name']}\")\n",
    "\n",
    "        # Load the model\n",
    "        model_obj = HuggingFaceInference(model['Model'])\n",
    "\n",
    "        # Get the prompts\n",
    "        inputs = dataset['Dataset']['input']\n",
    "\n",
    "        # TODO: We should maybe vary this somehow\n",
    "        # Map the inputs with the task prefix\n",
    "        prompts = [TaskPrefix.INSTRUCTION_AT_TOP.value + \"\\n\" + input for input in inputs]\n",
    "\n",
    "        # Get the gold labels\n",
    "        summaries = dataset['Dataset']['summary']\n",
    "\n",
    "        # Get the model responses\n",
    "        model_responses = model_obj.predict_batch(prompts, model['Options'] if model['Options'] else {})\n",
    "\n",
    "        # Grab the min-k% responses and calculations \n",
    "        min_k_responses = detect_pretraining_batch(summaries, model['Model'])\n",
    "\n",
    "        # Save the results\n",
    "        model_results_row = create_model_results_row()\n",
    "        model_results_row['Model'] = model['Model']\n",
    "        model_results_row['Task_Prefix'] = TaskPrefix.INSTRUCTION_AT_TOP.value\n",
    "        model_results_row['Dataset_Name'] = dataset['Name']\n",
    "        model_results_row['Model_Responses'] = model_responses\n",
    "        model_results_row['Min_K_Responses'] = min_k_responses\n",
    "        \n",
    "        model_results_row['Gold_Labels'] = summaries\n",
    "\n",
    "        print(f\"Now calculating cos/rouge scores for run {run_key}\")\n",
    "        for (prediction, gold_label) in zip(model_results_row['Model_Responses'], model_results_row['Gold_Labels']):\n",
    "            model_results_row['cos_sim_scores'].append(get_cos_similarity_score(embedder.encode(prediction), embedder.encode(gold_label)))\n",
    "            model_results_row['rouge_sim_scores'].append(get_rouge_scores(prediction, gold_label))\n",
    "\n",
    "        print(f\"Attempting to save file for run {run_key}....\")\n",
    "        file_path_prefix = \"./output/\"\n",
    "        file_name = run_key.replace(\"//\", \"_\").replace(\"/\", \"_\")\n",
    "        full_file_path = f\"{file_path_prefix}/{file_name}\"\n",
    "        save_dict(model_results_row, full_file_path)\n",
    "    \n",
    "        # Print how long it took\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds for run {run_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
