{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from SimilarityHelpers import get_cos_similarity_score, get_rouge_scores\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from detect_pretraining import detect_pretraining_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Steps:\n",
    "- Load the dataset(s)\n",
    "- (Optional) Do any splitting if needed\n",
    "- (Optional) Filter any low quality input/output pairs\n",
    "- Rename all article/text/content/body columns to \"input\"\n",
    "- Rename all summary/label/output columns to \"summary\" \n",
    "\n",
    "\n",
    "Notes:\n",
    "- Renaming is done so its easier during inference time to make model calls\n",
    "- Some datasets are too large to load reliably so we split and use only a portion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_WEBIS = False\n",
    "LOAD_SAMSUM = True\n",
    "LOAD_CNNDM = True\n",
    "LOAD_XSUM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'summary'],\n",
       "    num_rows: 819\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LOAD_SAMSUM:\n",
    "    samsum = load_dataset(\"Samsung/samsum\", trust_remote_code=True)['test']\n",
    "    samsum = samsum.rename_columns({\"dialogue\": \"input\"})\n",
    "\n",
    "samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'summary', 'id'],\n",
       "    num_rows: 1701\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum = load_dataset(\"EdinburghNLP/xsum\", trust_remote_code=True)['test']\n",
    "xsum_split = xsum.shuffle(seed=random_seed).train_test_split(test_size=0.15)\n",
    "xsum_test = xsum_split['test']\n",
    "xsum_test = xsum_test.rename_columns({\"document\": \"input\"})\n",
    "xsum_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'summary', 'id'],\n",
       "    num_rows: 1379\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LOAD_CNNDM:\n",
    "    cnn = load_dataset(\"ccdv/cnn_dailymail\", \"3.0.0\", trust_remote_code=True)\n",
    "    cnn_split = cnn['test'].shuffle(seed=random_seed).train_test_split(test_size=0.12)\n",
    "    cnn_test = cnn_split['test']\n",
    "\n",
    "    cnn_test = cnn_test.rename_columns({\"article\": \"input\"})\n",
    "    cnn_test = cnn_test.rename_columns({\"highlights\": \"summary\"})\n",
    "\n",
    "cnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LOAD_WEBIS:\n",
    "    webis = load_dataset(\"webis/tldr-17\", trust_remote_code=True)\n",
    "\n",
    "    # Take a random 10% of Webis\n",
    "    webis_sel = webis['train'].shuffle(seed=random_seed).train_test_split(test_size=0.01)\n",
    "    webis_test = webis_sel['test']\n",
    "\n",
    "\n",
    "    # Normalize the dataset input/output columns so it is input for the model input and summary for the gold label\n",
    "    webis_test = webis_test.rename_columns({\"content\": \"input\"})\n",
    "\n",
    "    # Trigger garbage collection\n",
    "    webis = None\n",
    "    webis_sel = None\n",
    "\n",
    "    webis_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefix(Enum):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    INSTRUCTION_AT_TOP = \"Summarize the following text: \"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Enum):\n",
    "    \"\"\"\n",
    "    Models supported by HuggingFace we can use for testing.\n",
    "    We picked models that come in multiple sizes.\n",
    "    - HuggingFaceTB/SmolLM2-135M\n",
    "    \"\"\"\n",
    "    SMOL_LM2_135M = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "    SMOL_LM2_135M_INSTRUCT = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "    SMOL_LM2_360M_INSTRUCT = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    SMOL_LM2_1p7B_INSTRUCT = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    \n",
    "    VICUNA_7B = \"lmsys/vicuna-7b-v1.5\"\n",
    "    VICUNA_13B = \"lmsys/vicuna-13b-v1.3\"\n",
    "\n",
    "    PHI_3p5_MINI_INSTRUCT = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "    PHI_3_SMALL_128K_INSTRUCT = \"microsoft/Phi-3-small-128k-instruct\"\n",
    "    PHI_3_MEDIUM_128K_INSTRUCT = \"microsoft/Phi-3-medium-128k-instruct\"\n",
    "\n",
    "    QWEN2p5_0p5B_INSTRUCT = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    QWEN2p5_1p5B_INSTRUCT = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "    QWEN2p5_3B_INSTRUCT = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    QWEN2p5_7B_INSTRUCT = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    QWEN2p5_14B_INSTRUCT = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "    QWEN2p5_32B_INSTRUCT = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "\n",
    "    LLAMA2_7B_CHAT = \"meta-llama/Llama-2-7b-chat\"\n",
    "    LLAMA2_13B_CHAT = \"meta-llama/Llama-2-13b-chat\"\n",
    "\n",
    "    LLAMA3p2_1B_INSTRUCT = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    LLAMA3p2_3B_INSTRUCT = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    LLAMA3p1_8B_INSTRUCT = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    LLAMA3p1_70B_INSTRUCT = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "    DEEPSEEK_R1_DISTILL_QWEN_1p5B = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    DEEPSEEK_R1_DISTILL_QWEN_7B   = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "    DEEPSEEK_R1_DISTILL_QWEN_14B  = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "    DEEPSEEK_R1_DISTILL_QWEN_32B  = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "\n",
    "    # OPT Series\n",
    "    OPT_125M = \"facebook/opt-125m\"\n",
    "    OPT_350M = \"facebook/opt-350m\" \n",
    "    OPT_1B = \"facebook/opt-1.3b\"\n",
    "    OPT_2B = \"facebook/opt-2.7b\"\n",
    "    OPT_6B = \"facebook/opt-6.7b\"\n",
    "    OPT_13B = \"facebook/opt-13b\"\n",
    "\n",
    "    # FLAN-T5 Progression\n",
    "    FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "    FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "    FLAN_T5_LARGE = \"google/flan-t5-large\" \n",
    "    FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "    FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "\n",
    "    # MobileLLM Variants\n",
    "    MOBILE_LLM_125M = \"facebook/mobilellm-125m\"\n",
    "    MOBILE_LLM_350M = \"facebook/mobilellm-350m\"\n",
    "\n",
    "    # Cerebras Models\n",
    "    BTLM_3B = \"cerebras/btlm-3b-8k-base\"\n",
    "\n",
    "    # GPT-Neo Series\n",
    "    GPT_NEO_125M = \"EleutherAI/gpt-neo-125m\"\n",
    "    GPT_NEO_1B = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    GPT_NEO_2B = \"EleutherAI/gpt-neo-2.7B\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceInference:\n",
    "    def __init__(self, model: Model):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model.value)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model.value).to(DEVICE)\n",
    "\n",
    "        # Add a padding token to the tokenizer if it doesn't exist\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "\n",
    "    def predict(self,  prompt: str, options: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the output of a model given a prompt using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {}\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            # Tokenize and generate\n",
    "            if not self.tokenizer.chat_template:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "            else:\n",
    "                input_text = self.tokenizer.apply_chat_template(messages,  tokenize=False)\n",
    "\n",
    "                inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                temperature=options.get(\"temperature\", 0.001),\n",
    "                top_p=options.get(\"top_p\", 0.9),\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "            # Extract just the new generated text, not the original prompt\n",
    "            prompt_length = len(inputs[\"input_ids\"][0])\n",
    "            generated_ids = outputs[0][prompt_length:]\n",
    "\n",
    "            return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during prediction: {e}\")\n",
    "            return \"None\"\n",
    "\n",
    "    def predict_batch(self, prompts: List[str], options: Optional[Dict] = {}) -> List[str]:\n",
    "        \"\"\"\n",
    "        Batch prediction using HuggingFace transformers.\n",
    "        \"\"\"\n",
    "        return [self.predict(prompt, options) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assistant\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj = HuggingFaceInference(Model.SMOL_LM2_135M_INSTRUCT)\n",
    "model_obj.predict(\"Capital of France?\", options={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_results_row():\n",
    "    return {\n",
    "        \"Model\": None,\n",
    "        \"Task_Prefix\": None,\n",
    "        \"Dataset_Name\": None,\n",
    "        \"Model_Responses\": None,\n",
    "        \"Gold_Labels\": None,\n",
    "        \"Min_K_Responses\": [],\n",
    "        \"cos_sim_scores\": [],\n",
    "        \"rouge_sim_scores\": []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_encoder(obj):\n",
    "    \"\"\"\n",
    "    Custom JSON encoder to handle types that the default JSON encoder\n",
    "    cannot handle, like NumPy arrays and PyTorch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    # If an object has a 'tolist' method, use it\n",
    "    if hasattr(obj, 'tolist'):\n",
    "        return obj.tolist()\n",
    "    # If all else fails, raise a TypeError\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(data, filename=\"output\"):\n",
    "    # Attempt to save as CSV using pandas DataFrame\n",
    "    try:\n",
    "        # Wrap the dict in a list to create a single-row DataFrame.\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(f\"{filename}.csv\", index=False)\n",
    "        print(f\"Data saved as CSV: {filename}.csv\")\n",
    "        return\n",
    "    except Exception as e_csv:\n",
    "        print(\"Saving as CSV failed:\", e_csv)\n",
    "\n",
    "    # Attempt to save as JSON\n",
    "    try:\n",
    "        with open(f\"{filename}.json\", \"w\") as f:\n",
    "            json.dump(data, f, default=custom_encoder)\n",
    "        print(f\"Data saved as JSON: {filename}.json\")\n",
    "        return\n",
    "    except Exception as e_json:\n",
    "        print(\"Saving as JSON failed:\", e_json)\n",
    "\n",
    "    # Attempt to save as Pickle\n",
    "    try:\n",
    "        with open(f\"{filename}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Data saved as Pickle: {filename}.pkl\")\n",
    "    except Exception as e_pickle:\n",
    "        print(\"Saving as Pickle failed:\", e_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EVALS = [\n",
    "    {\n",
    "        \"Model\": Model.SMOL_LM2_360M_INSTRUCT,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.GPT_NEO_125M,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.OPT_125M,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "    {\n",
    "        \"Model\": Model.OPT_350M,\n",
    "        \"Options\": {}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_EVALS = [\n",
    "#    {\n",
    "#        \"Name\": \"Samsum\",\n",
    "#        \"Dataset\": samsum,\n",
    "#    }\n",
    "#    {\n",
    "#        \"Name\": \"Webis\",\n",
    "#        \"Dataset\": webis_test,\n",
    "#    }\n",
    "#    {\n",
    "#        \"Name\": \"CNN-DailyMail\",\n",
    "#        \"Dataset\": cnn_test,\n",
    "#    }\n",
    "        {\n",
    "        \"Name\": \"Xsum\",\n",
    "        \"Dataset\": xsum_test,\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = {} # Used for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Evaluating HuggingFaceTB/SmolLM2-360M-Instruct on Xsum\n",
      "Generating Min-K metrics.....\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASET_EVALS:\n",
    "    for model in MODEL_EVALS:\n",
    "        run_key = f\"{model['Model']}_{dataset['Name']}\"\n",
    "\n",
    "        # Start a timer\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(f\"Evaluating {model['Model']} on {dataset['Name']}\")\n",
    "\n",
    "        # Load the model\n",
    "        model_obj = HuggingFaceInference(model['Model'])\n",
    "\n",
    "        # Get the prompts\n",
    "        inputs = dataset['Dataset']['input']\n",
    "\n",
    "        # TODO: We should maybe vary this somehow\n",
    "        # Map the inputs with the task prefix\n",
    "        prompts = [TaskPrefix.INSTRUCTION_AT_TOP.value + \"\\n\" + input for input in inputs]\n",
    "\n",
    "        # Get the gold labels\n",
    "        summaries = dataset['Dataset']['summary']\n",
    "\n",
    "        # Grab the min-k% responses and calculations\n",
    "        print(\"Generating Min-K metrics.....\")\n",
    "        min_k_responses = detect_pretraining_batch(summaries, model_obj)\n",
    "\n",
    "        # Get the model responses\n",
    "        print(\"Running inference over inputs.....\")\n",
    "        model_responses = model_obj.predict_batch(prompts, model['Options'] if model['Options'] else {})\n",
    "\n",
    "        # Save the results\n",
    "        model_results_row = create_model_results_row()\n",
    "        model_results_row['Model'] = model['Model']\n",
    "        model_results_row['Task_Prefix'] = TaskPrefix.INSTRUCTION_AT_TOP.value\n",
    "        model_results_row['Dataset_Name'] = dataset['Name']\n",
    "        model_results_row['Model_Responses'] = model_responses\n",
    "        model_results_row['Min_K_Responses'] = min_k_responses\n",
    "        \n",
    "        model_results_row['Gold_Labels'] = summaries\n",
    "\n",
    "        print(f\"Calculating cos/rouge scores for run {run_key}\")\n",
    "        for (prediction, gold_label) in zip(model_results_row['Model_Responses'], model_results_row['Gold_Labels']):\n",
    "            model_results_row['cos_sim_scores'].append(get_cos_similarity_score(embedder.encode(prediction), embedder.encode(gold_label)))\n",
    "            model_results_row['rouge_sim_scores'].append(get_rouge_scores(prediction, gold_label))\n",
    "\n",
    "        print(f\"Attempting to save file for run {run_key}....\")\n",
    "        file_path_prefix = \"./output/\"\n",
    "        file_name = run_key.replace(\"//\", \"_\").replace(\"/\", \"_\")\n",
    "        full_file_path = f\"{file_path_prefix}/{file_name}\"\n",
    "        save_dict(model_results_row, full_file_path)\n",
    "    \n",
    "        # Print how long it took\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds for run {run_key}\")\n",
    "\n",
    "        last_run = model_results_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as CSV: ./output//HuggingFaceTB_SmolLM2-135M-Instruct_Samsum.csv\n"
     ]
    }
   ],
   "source": [
    "save_dict(last_run, full_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
